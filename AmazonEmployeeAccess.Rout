
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> #install.packages('tidyverse')
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> #install.packages('tidymodels')
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
✔ recipes      1.0.8     
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ scales::discard() masks purrr::discard()
✖ dplyr::filter()   masks stats::filter()
✖ recipes::fixed()  masks stringr::fixed()
✖ dplyr::lag()      masks stats::lag()
✖ yardstick::spec() masks readr::spec()
✖ recipes::step()   masks stats::step()
• Use suppressPackageStartupMessages() to eliminate package startup messages
> #install.packages('DataExplorer')
> #install.packages("poissonreg")
> # library(poissonreg)
> #install.packages("glmnet")
> library(glmnet)
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

Loaded glmnet 4.1-8
> #library(patchwork)
> # install.packages("rpart")
> #install.packages('ranger')
> library(ranger)
> #install.packages('stacks')
> library(stacks)
> #install.packages('vroom')
> library(vroom)

Attaching package: ‘vroom’

The following object is masked from ‘package:yardstick’:

    spec

The following object is masked from ‘package:scales’:

    col_factor

The following objects are masked from ‘package:readr’:

    as.col_spec, col_character, col_date, col_datetime, col_double,
    col_factor, col_guess, col_integer, col_logical, col_number,
    col_skip, col_time, cols, cols_condense, cols_only, date_names,
    date_names_lang, date_names_langs, default_locale, fwf_cols,
    fwf_empty, fwf_positions, fwf_widths, locale, output_column,
    problems, spec

> #install.packages('parsnip')
> library(parsnip)
> # install.packages('dbarts')
> # library(dbarts)
> #install.packages('embed')
> library(embed)
> 
> # rm(list=ls()) use to erase environment
> 
> ## 112 Cols
> 
> data_train <- vroom("./data/train.csv") %>%
+   mutate(ACTION=factor(ACTION))# grab training data
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> 
> data_train
# A tibble: 32,769 × 10
   ACTION RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
   <fct>     <dbl>  <dbl>         <dbl>         <dbl>         <dbl>      <dbl>
 1 1         39353  85475        117961        118300        123472     117905
 2 1         17183   1540        117961        118343        123125     118536
 3 1         36724  14457        118219        118220        117884     117879
 4 1         36135   5396        117961        118343        119993     118321
 5 1         42680   5905        117929        117930        119569     119323
 6 0         45333  14561        117951        117952        118008     118568
 7 1         25993  17227        117961        118343        123476     118980
 8 1         19666   4209        117961        117969        118910     126820
 9 1         31246    783        117961        118413        120584     128230
10 1         78766  56683        118079        118080        117878     117879
# ℹ 32,759 more rows
# ℹ 3 more variables: ROLE_FAMILY_DESC <dbl>, ROLE_FAMILY <dbl>,
#   ROLE_CODE <dbl>
> 
> 
> ###############
> ##### EDA #####
> ###############
> 
> # library(ggplot2)
> # 
> # boxplot(data_train$ROLE_CODE ~ data_train$ACTION,
> #         col='steelblue',
> #         main='action by role code',
> #         xlab='Action',
> #         ylab='ROLE_CODE')
> # 
> # boxplot(data_train$ROLE_TITLE ~ data_train$ACTION,
> #         col='steelblue',
> #         main='action by role title',
> #         xlab='Action',
> #         ylab='ROLE_TITLE')
> 
> 
> #######################
> ##### Recipe/Bake #####
> #######################
> 
> rFormula <- ACTION ~ .
> 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .01) %>% # get hours
> #   step_dummy(all_nominal_predictors()) # get dummy variables
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> 
> ### For target encoding/Random Forests: ###
> 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .001) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))# get hours
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> 
> # ncol(baked_data1)
> 
> 
> ##################################################
> ##### Logistic Regression: Bin Cross Entropy #####
> ##################################################
> 
> # log_reg <- logistic_reg() %>% #Type of model
> #   set_engine("glm")
> # 
> # amazon_workflow <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(log_reg) %>%
> #   fit(data = data_train) # Fit the workflow
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(amazon_workflow,
> #                          new_data=data_test,
> #                          type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "amazon_pred_logreg.csv", delim = ",")
> # save(file = 'amazon_wf.RData', list = c('amazon_workflow'))
> # load('amazon_wf.RData')
> 
> 
> ################################################
> ##### Logistic Regression: target encoding #####
> ################################################
> 
> # rFormula <- ACTION ~ .
> # 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .001) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))# get hours
> #   
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> # 
> # log_reg <- logistic_reg(mixture = tune(), penalty = tune()) %>% #Type of model
> #   set_engine("glmnet")
> # 
> # pretune_workflow <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(log_reg)
> # 
> # # Grid for CV
> # tuning_grid <- grid_regular(penalty(),
> #                             mixture(),
> #                             levels = 5) ## L^2 tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- pretune_workflow %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- pretune_workflow %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "amazon_logreg_target.csv", delim = ",")
> # save(file = 'amazon_penalized_wf.RData', list = c('final_wf'))
> # load('amazon_penalized_wf.RData')
> 
> 
> 
> ########################################
> ##### Classification Random Forest #####
> ########################################
> 
> # class_rf_mod <- rand_forest(mtry = tune(), 
> #                             min_n = tune(),
> #                             trees = 800) %>% #Type of model
> #   set_engine('ranger') %>%
> #   set_mode('classification')
> # 
> # pretune_workflow <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(class_rf_mod)
> # 
> # ## Grid of values to tune over
> # tuning_grid <- grid_regular(mtry(range = c(1,ncol(data_train)-1)),
> #                             min_n(),
> #                             levels = 3) ## L^2 total tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- pretune_workflow %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- pretune_workflow %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   #mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   mutate(ACTION = .pred_1) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "amazon_pred_rf.csv", delim = ",")
> # save(file = 'amazon_penalized_wf.RData', list = c('final_wf'))
> # load('amazon_penalized_wf.RData')
> 
> 
> 
> ################################
> ##### Naive Bayes Approach #####
> ################################
> 
> # install.packages('discrim')
> # library(discrim)
> # install.packages('naivebayes')
> # library(naivebayes)
> # 
> # # nb model
> # nb_mod <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
> #                         set_mode('classification') %>%
> #                         set_engine('naivebayes')
> # 
> # nb_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(nb_mod)
> # 
> # tuning_grid <- grid_regular(Laplace(),
> #                             smoothness(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- nb_wf %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- nb_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   #mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   mutate(ACTION = .pred_1) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "amazon_pred_nb.csv", delim = ",")
> # save(file = 'amazon_penalized_wf.RData', list = c('final_wf'))
> # load('amazon_penalized_wf.RData')
> 
> 
> 
> ###############################
> ##### K-Nearest Neighbors #####
> ###############################
> 
> # install.packages('kknn')
> # library(kknn)
> # 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .001) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% # get hours
> #   step_normalize()
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> # 
> # ## knn model
> # knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
> #   set_mode("classification") %>%
> #   set_engine("kknn")
> # 
> # knn_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(knn_model)
> # 
> # ## Fit or Tune MOdel
> # tuning_grid <- grid_regular(neighbors(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- knn_wf %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- knn_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   #mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   mutate(ACTION = .pred_1) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "amazon_pred_knn.csv", delim = ",")
> 
> 
> ####################################################
> ##### Naive Bayes Principle Comp Dim Reduction #####
> ####################################################
> 
> # install.packages('discrim')
> # library(discrim)
> # install.packages('naivebayes')
> # library(naivebayes)
> # 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .001) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))%>% # get hours
> #   step_normalize(all_predictors()) %>%
> #   step_pca(all_predictors(), threshold = 0.9) # Threshold between 0 and 1
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> # 
> # # nb model
> # nb_mod <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
> #   set_mode('classification') %>%
> #   set_engine('naivebayes')
> # 
> # nb_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(nb_mod)
> # 
> # tuning_grid <- grid_regular(Laplace(),
> #                             smoothness(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- nb_wf %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- nb_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   #mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   mutate(ACTION = .pred_1) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "./data/amazon_nb_dim_red.csv", delim = ",")
> # save(file = 'amazon_penalized_wf.RData', list = c('final_wf'))
> # load('amazon_penalized_wf.RData')
> 
> 
> ##################################
> ##### KNN Comp Dim Reduction #####
> ##################################
> 
> # install.packages('kknn')
> # library(kknn)
> # 
> # my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_other(all_nominal_predictors(), threshold = .001) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% # get hours
> #   step_normalize(all_predictors()) %>%
> #   step_pca(all_predictors(), threshold = 0.9) # Threshold between 0 and 1
> # 
> # prepped_recipe <- prep(my_recipe) # preprocessing new data
> # baked_data1 <- bake(prepped_recipe, new_data = data_train)
> # 
> # ## knn model
> # knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
> #   set_mode("classification") %>%
> #   set_engine("kknn")
> # 
> # knn_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(knn_model)
> # 
> # ## Fit or Tune Model
> # tuning_grid <- grid_regular(neighbors(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # # Split data for CV
> # folds <- vfold_cv(data_train, v = 10, repeats = 1)
> # 
> # # Run CV
> # CV_results <- knn_wf %>%
> #   tune_grid(resamples = folds,
> #             grid = tuning_grid,
> #             metrics = metric_set(roc_auc))
> # 
> # bestTune <- CV_results %>%
> #   select_best('roc_auc')
> # 
> # final_wf <- knn_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = data_train)
> # 
> # data_test <- vroom("./data/test.csv") # grab testing data
> # 
> # amazon_predictions <- predict(final_wf,
> #                               new_data=data_test,
> #                               type="prob") %>% # "class" or "prob"
> #   mutate(Id = data_test$id) %>%
> #   #mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
> #   mutate(ACTION = .pred_1) %>%
> #   select(-.pred_0, -.pred_1)
> # 
> # vroom_write(amazon_predictions, "./data/amazon_knn_dim_red.csv", delim = ",")
> 
> 
> ################
> ##### SVMS #####
> ################
> 
> # target encoding
> my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
+     step_mutate_at(all_numeric_predictors(), fn = factor) %>%
+     step_other(all_nominal_predictors(), threshold = .001) %>%
+     step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% # get hours
+     step_normalize(all_predictors())
> 
> prepped_recipe <- prep(my_recipe) # preprocessing new data
> baked_data1 <- bake(prepped_recipe, new_data = data_train)
> 
> ## SVM models
> svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> svmRadial <- svm_rbf(rbf_sigma=tune(), cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> svmLinear <- svm_linear(cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> svm_wf <- workflow() %>%
+   add_recipe(my_recipe) %>%
+   add_model(svmRadial)
> 
> ## Fit or Tune Model
> tuning_grid <- grid_regular(rbf_sigma(),
+                             cost(),
+                             levels = 2) ## L^2 total tuning possibilities
> 
> # Split data for CV
> folds <- vfold_cv(data_train, v = 5, repeats = 1)
> 
> # Run CV
> CV_results <- svm_wf %>%
+   tune_grid(resamples = folds,
+             grid = tuning_grid,
+             metrics = metric_set(roc_auc))
