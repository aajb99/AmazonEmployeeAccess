xlab='X', ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = "), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(0, 7), ylim=c(0, 1.5),
xlab='X', ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
gamma_1 <- 0.5
phi_1 <- 0.5
exp_val1 <- gamma_1 / phi_1
exp_val1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 0.5"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
# Prior distribution vs posterior distribution
curve(dbeta(x, gamma_1, phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="density",
main="Prior vs Posterior Distributions for # of Potholes per Block",
lwd=2)
curve(dbeta(x, gamma_post, phi_post), add=T, col="blue", lwd=2)
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "blue"),
lwd=2, lty=c(1,1))
# Prior distribution vs posterior distribution
curve(dgamma(x, gamma_1, phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="density",
main="Prior vs Posterior Distributions for # of Potholes per Block",
lwd=2)
curve(dgamma(x, gamma_post, phi_post), add=T, col="blue", lwd=2)
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "blue"),
lwd=2, lty=c(1,1))
knitr::opts_chunk$set(echo = TRUE)
#uncomment any of these that you will want to use (and add any others)
#install.packages('tidyverse')
#install.packages('rlang')
#install.packages('moments')
# remove.packages(rlange)
library(tinytex)
library(tidyverse)
library(ggplot2)
library(moments)
gamma_1 <- 0.5
phi_1 <- 0.5
exp_val1 <- gamma_1 / phi_1
exp_val1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 0.5"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
# Prior distribution vs posterior distribution
curve(dgamma(x, gamma_1, phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="density",
main="Prior vs Posterior Distributions for # of Potholes per Block",
lwd=2)
curve(dgamma(x, gamma_post, phi_post), add=T, col="blue", lwd=2)
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "blue"),
lwd=2, lty=c(1,1))
exp_val_post <- gamma_post / phi_post
exp_val_post
# 95% Credible Interval
qgamma(c(.025, .975), gamma_post, phi_post)
# Posterior probability that the average number of potholes is greater than 4:
pgamma(4, gamma_post, phi_post, lower.tail=F)
# Posterior Predictive Distribution
# Monte Carlo approximation of pi(lambda|data)
lambda <- rgamma(10000, gamma_post, phi_post)
x_pred <- rpois(10000, lambda)
hist(x_pred, freq=F, ylim=c(0, 0.25), xlim=c(0, 15))
#posterior predictive probability that the number of potholes will be greater than 4 in a randomly selected block
mean(x_pred > 4)
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / phi_1
exp_val1
var1 <- gamma_1*(phi_1+1)/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8"), col=c("black"),
lwd=2, lty=1)
data_w <- c(12, 9, 10, 8, 9, 4, 10, 15, 3, 5, 11, 8, 9, 4, 2, 7, 9,
5, 4, 2, 3, 12, 10, 2, 9, 8, 13, 9, 7, 6, 6, 2, 2, 6, 8)
data_m <- c(2, 3, 0, 4, 1, 1, 1, 2, 2, 2, 0, 3, 2)
sum_w <- sum(data_w)
sum_m <- sum(data_m)
gamma.p.w <- gamma_1 + sum_w
gamma.p.w
phi.p.w <- phi_1 + 35
phi.p.w
gamma.p.m <- gamma_1 + sum_m
gamma.p.m
phi.p.m <- phi_1 + 13
phi.p.m
curve(dgamma(x, shape = gamma.p.w, rate = phi.p.w), xlim = c(0, 10), ylim=c(0, 1.15),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution for Women vs Men", col="red", lwd=2)
curve(dgamma(x, shape = gamma.p.m, rate = phi.p.m), add = T, col="blue", lwd=2)
legend("topright", legend=c("Women", "Men"), col=c("red", "blue"),
lwd=2, lty=c(1,1))
# 95% Credible Intervals
# Women
qgamma(c(.025, .975), gamma.p.w, phi.p.w)
# Men
qgamma(c(.025, .975), gamma.p.m, phi.p.m)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), cex.axis=cex.plots, cex.lab=cex.plots, lwd=2)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
print(diff)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
mean(diff)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
mean_d <- mean(diff)
mean_d
knitr::opts_chunk$set(echo = TRUE)
#uncomment any of these that you will want to use (and add any others)
#install.packages('tidyverse')
#install.packages('rlang')
#install.packages('moments')
# remove.packages(rlange)
library(tinytex)
library(tidyverse)
library(ggplot2)
library(moments)
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / (phi_1)^2
exp_val1
var1 <- gamma_1*(phi_1+1)/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8"), col=c("black"),
lwd=2, lty=1)
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / phi_1
exp_val1
var1 <- gamma_1*(phi_1+1)/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8"), col=c("black"),
lwd=2, lty=1)
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / phi_1
exp_val1
var1 <- gamma_1/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(1.5, 6), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(1.5, 6), ylim=c(0, 1.2),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
# Prior distribution vs posterior distribution
curve(dgamma(x, gamma_1, phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="density",
main="Prior vs Posterior Distributions for # of Potholes per Block",
lwd=2)
curve(dgamma(x, gamma_post, phi_post), add=T, col="blue", lwd=2)
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "blue"),
lwd=2, lty=c(1,1))
gamma_1 <- 0.5
phi_1 <- 0.5
exp_val1 <- gamma_1 / phi_1
exp_val1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 0.5"), col=c("black"),
lwd=2, lty=1)
data_1 <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
sum_x <- sum(data_1)
gamma_post <- gamma_1 + sum_x
gamma_post
phi_post <- phi_1 + 20
phi_post
curve(dgamma(x, shape = gamma_post, rate = phi_post), xlim = c(1.5, 6), ylim=c(0, 1.2),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 70.5, Rate = 20.5"), col=c("black"),
lwd=2, lty=1)
# Prior distribution vs posterior distribution
curve(dgamma(x, gamma_1, phi_1), xlim = c(0, 7), ylim=c(0, 1.5),
xlab=expression(lambda), ylab="density",
main="Prior vs Posterior Distributions for # of Potholes per Block",
lwd=2)
curve(dgamma(x, gamma_post, phi_post), add=T, col="blue", lwd=2)
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "blue"),
lwd=2, lty=c(1,1))
exp_val_post <- gamma_post / phi_post
exp_val_post
# 95% Credible Interval
qgamma(c(.025, .975), gamma_post, phi_post)
# Posterior probability that the average number of potholes is greater than 4:
pgamma(4, gamma_post, phi_post, lower.tail=F)
# Posterior Predictive Distribution
# Monte Carlo approximation of pi(lambda|data)
lambda <- rgamma(10000, gamma_post, phi_post)
x_pred <- rpois(10000, lambda)
hist(x_pred, freq=F, ylim=c(0, 0.25), xlim=c(0, 15))
#posterior predictive probability that the number of potholes will be greater than 4 in a randomly selected block
prob_1g <- mean(x_pred > 4)
# Posterior Predictive Distribution
# Monte Carlo approximation of pi(lambda|data)
lambda <- rgamma(10000, gamma_post, phi_post)
x_pred <- rpois(10000, lambda)
hist(x_pred, freq=F, ylim=c(0, 0.25), xlim=c(0, 15))
#posterior predictive probability that the number of potholes will be greater than 4 in a randomly selected block
prob_1g <- mean(x_pred > 4)
prob_1g
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / phi_1
exp_val1
var1 <- gamma_1/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8"), col=c("black"),
lwd=2, lty=1)
gamma_1 <- 8
phi_1 <- 1.5
exp_val1 <- gamma_1 / phi_1
exp_val1
var1 <- gamma_1/(phi_1^2)
var1
curve(dgamma(x, shape = gamma_1, rate = phi_1), xlim = c(0, 16), ylim=c(0, .3),
xlab=expression(lambda), ylab="Density", main="A priori Gamma Distribution", lwd=2)
legend("topright", legend=c("Shape = 8", "Rate = 1.5"), col=c("black"),
lwd=2, lty=1)
data_w <- c(12, 9, 10, 8, 9, 4, 10, 15, 3, 5, 11, 8, 9, 4, 2, 7, 9,
5, 4, 2, 3, 12, 10, 2, 9, 8, 13, 9, 7, 6, 6, 2, 2, 6, 8)
data_m <- c(2, 3, 0, 4, 1, 1, 1, 2, 2, 2, 0, 3, 2)
sum_w <- sum(data_w)
sum_m <- sum(data_m)
gamma.p.w <- gamma_1 + sum_w
gamma.p.w
phi.p.w <- phi_1 + 35
phi.p.w
gamma.p.m <- gamma_1 + sum_m
gamma.p.m
phi.p.m <- phi_1 + 13
phi.p.m
curve(dgamma(x, shape = gamma.p.w, rate = phi.p.w), xlim = c(0, 10), ylim=c(0, 1.15),
xlab=expression(lambda), ylab="Density", main="Posterior Gamma Distribution for Women vs Men", col="red", lwd=2)
curve(dgamma(x, shape = gamma.p.m, rate = phi.p.m), add = T, col="blue", lwd=2)
legend("topright", legend=c("Women", "Men"), col=c("red", "blue"),
lwd=2, lty=c(1,1))
# 95% Credible Intervals
# Women
qgamma(c(.025, .975), gamma.p.w, phi.p.w)
# Men
qgamma(c(.025, .975), gamma.p.m, phi.p.m)
# Posterior Dist on d = lambda_W - lambda_M:
# Monte Carlo Approx. to determine distribution of diff|data:
lambda.w <- rgamma(10000, gamma.p.w, phi.p.w)
lambda.m <- rgamma(10000, gamma.p.m, phi.p.m)
diff <- lambda.w - lambda.m
#Plot of the posterior on the difference
plot(density(diff), xlab=expression(lambda[W] - lambda[M]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[W]-lambda[M])), lwd=2)
abline(v=0, lty=2)
mean_d <- mean(diff)
mean_d
install.packages('themis')
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
qnorm(.95)
qnorm(.995)
qnorm(.9)
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
setwd("~/byu_fall_2023/Stat_348/STAT348/AmazonEmployeeAccess")
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
rFormula <- ACTION ~ .
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #%>% # get hours
#step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
#step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
# ncol(baked_data1)
view(baked_data1)
my_recipe
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
rFormula <- ACTION ~ .
## For target encoding/Random Forests: ###
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) #%>%
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
## For target encoding/Random Forests: ###
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #%>% # get hours
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
#######################
##### Recipe/Bake #####
#######################
rFormula <- ACTION ~ .
## For target encoding/Random Forests: ###
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% # get hours
step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
rFormula <- ACTION ~ .
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5) %>%
step_normalize()
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_pca(all_predictors(), threshold = 0.8)
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
my_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
#step_pca(all_predictors(), threshold = 0.8) #%>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(my_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
